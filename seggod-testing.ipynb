{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12818800,"sourceType":"datasetVersion","datasetId":8106076}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/viswanathperala/seggod-testing?scriptVersionId=257261980\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q torch\n!pip install -q pillow\n!pip install -q transformers\n!pip install -q torch torchvision torchaudio\n!pip install -q opencv-python matplotlib numpy\n!pip install -q git+https://github.com/facebookresearch/segment-anything.git\n!pip install -q git+https://github.com/openai/CLIP.git\n!pip install -q Pillow ftfy regex tqdm\n!pip install -q open_clip_torch\n!pip install -q git+https://github.com/tulip-berkeley/open_clip.git\n!pip install -q timm --upgrade\n!pip install -q scipy\n!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\n!wget http://farm7.staticflickr.com/6128/5979928997_37d5092842_z.jpg\n!wget https://s3.us-west-1.wasabisys.com/tulip/ckpt-s1/tulip-B-16-224.ckpt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nSegGOD Batch Processor\nModified to process multiple images and save bounding boxes in JSON format\nfor IoU evaluation against manual annotations\n\"\"\"\n\nimport torch\nimport numpy as np\nimport cv2\nimport json\nimport os\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw, ImageFont\nfrom transformers import AutoProcessor, AutoModel\nimport torch.nn.functional as F\nfrom segment_anything import SamAutomaticMaskGenerator, sam_model_registry\nimport math\n\nclass SegGODBatchProcessor:\n    def __init__(self, \n                 sam_model_path=\"sam_vit_l_0b3195.pth\",\n                 vlm_model=\"google/siglip-base-patch16-224\",\n                 device=None):\n        \"\"\"\n        Initialize SegGOD framework for batch processing\n        \n        Args:\n            sam_model_path: Path to SAM model checkpoint\n            vlm_model: VLM model name (SigLIP recommended)\n            device: Compute device (auto-detected if None)\n        \"\"\"\n        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Load SAM model\n        print(\"Loading SAM model...\")\n        sam = sam_model_registry[\"vit_l\"](checkpoint=sam_model_path)\n        sam.to(device=self.device)\n        self.sam_generator = SamAutomaticMaskGenerator(sam)\n        \n        # Load VLM model (SigLIP)\n        print(\"Loading SigLIP model...\")\n        self.processor = AutoProcessor.from_pretrained(vlm_model)\n        self.model = AutoModel.from_pretrained(vlm_model).to(self.device).eval()\n        self.image_count = 0\n        \n        print(f\"SegGOD initialized on {self.device}\")\n    \n    def stage1_sam_segmentation(self, image):\n        \"\"\"STAGE 1: Generate segment masks using SAM\"\"\"\n        masks = self.sam_generator.generate(image)\n        return masks\n    \n    def stage2_object_proposals(self, image, masks, method=\"morphological\", \n                               kernel_size=15, iterations=2):\n        \"\"\"STAGE 2: Generate object proposals using morphological operations\"\"\"\n        proposals = []\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (kernel_size, kernel_size))\n        \n        for mask_data in masks:\n            mask = mask_data['segmentation'].astype(np.uint8)\n            \n            if np.sum(mask) == 0:\n                continue\n            \n            # Apply morphological operations\n            dilated = cv2.dilate(mask, kernel, iterations=iterations)\n            closed = cv2.morphologyEx(dilated, cv2.MORPH_CLOSE, kernel)\n            \n            # Get bounding box\n            coords = np.where(closed)\n            if len(coords[0]) > 0:\n                y_min, y_max = coords[0].min(), coords[0].max()\n                x_min, x_max = coords[1].min(), coords[1].max()\n                \n                # Crop the region\n                cropped = image[y_min:y_max+1, x_min:x_max+1]\n                bbox = (x_min, y_min, x_max, y_max)\n                proposals.append((cropped, bbox))\n        \n        return proposals\n    \n    def stage3_content_aware_interpolation(self, proposals, target_size=224, min_size=32):\n        \"\"\"STAGE 3: Content-aware interpolation\"\"\"\n        processed_proposals = []\n        \n        for cropped, bbox in proposals:\n            if cropped.shape[0] < min_size or cropped.shape[1] < min_size:\n                continue\n            \n            height, width = cropped.shape[:2]\n            aspect_ratio = width / height\n            \n            # Content-aware strategy based on aspect ratio\n            if aspect_ratio > 2.0:  # Very wide objects\n                new_height = width\n                resized = cv2.resize(cropped, (width, new_height))\n                \n                padded = np.zeros((width, width, 3), dtype=np.uint8)\n                y_offset = (width - new_height) // 2\n                padded[y_offset:y_offset+new_height, :] = resized\n                \n                if y_offset > 0:\n                    padded[:y_offset, :] = resized[0:1, :]\n                    padded[y_offset+new_height:, :] = resized[-1:, :]\n                \n            elif aspect_ratio < 0.5:  # Very tall objects\n                new_width = height\n                resized = cv2.resize(cropped, (new_width, height))\n                \n                padded = np.zeros((height, height, 3), dtype=np.uint8)\n                x_offset = (height - new_width) // 2\n                padded[:, x_offset:x_offset+new_width] = resized\n                \n                if x_offset > 0:\n                    padded[:, :x_offset] = resized[:, 0:1]\n                    padded[:, x_offset+new_width:] = resized[:, -1:]\n                    \n            else:  # Roughly square objects\n                max_dim = max(height, width)\n                padded = np.zeros((max_dim, max_dim, 3), dtype=np.uint8)\n                \n                y_offset = (max_dim - height) // 2\n                x_offset = (max_dim - width) // 2\n                padded[y_offset:y_offset+height, x_offset:x_offset+width] = cropped\n                \n                if y_offset > 0:\n                    padded[:y_offset, x_offset:x_offset+width] = cropped[0:1, :]\n                    padded[y_offset+height:, x_offset:x_offset+width] = cropped[-1:, :]\n                if x_offset > 0:\n                    padded[:, :x_offset] = padded[:, x_offset:x_offset+1]\n                    padded[:, x_offset+width:] = padded[:, x_offset+width-1:x_offset+width]\n            \n            # Resize to target size and convert to PIL\n            final = cv2.resize(padded, (target_size, target_size))\n            final_pil = Image.fromarray(final)\n            \n            processed_proposals.append((final_pil, bbox))\n        \n        return processed_proposals\n    \n    def stage4_vlm_processing(self, processed_proposals, text_query):\n        \"\"\"STAGE 4: VLM processing using SigLIP\"\"\"\n        # Get text embedding\n        text_inputs = self.processor.tokenizer(\n            [text_query],\n            padding=\"max_length\",\n            truncation=True,\n            max_length=64,\n            return_tensors=\"pt\"\n        ).to(self.device)\n        \n        with torch.no_grad():\n            text_embeds = self.model.get_text_features(**text_inputs)\n            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n        \n        scored_detections = []\n        \n        for image_pil, bbox in processed_proposals:\n            # Get image embedding\n            img_inputs = self.processor.image_processor(\n                [image_pil],\n                return_tensors=\"pt\"\n            ).to(self.device)\n            \n            with torch.no_grad():\n                img_embeds = self.model.get_image_features(**img_inputs)\n                img_embeds = img_embeds / img_embeds.norm(dim=-1, keepdim=True)\n            \n            # Calculate similarity\n            similarity = F.cosine_similarity(text_embeds, img_embeds).item()\n            scored_detections.append((bbox, similarity))\n        \n        return scored_detections\n    \n    def stage5_post_processing(self, scored_detections, threshold=0.1):\n        \"\"\"STAGE 5: Post-processing with threshold filtering\"\"\"\n        final_detections = [(bbox, score) for bbox, score in scored_detections if score > threshold]\n        return final_detections\n\n    def stage6_visualization(self, original_image, final_detections, processed_proposals, \n                           scored_detections, text_query, save_prefix=\"seggod\"):\n        \"\"\"\n        STAGE 6: Visualization of results\n        \n        Args:\n            original_image: Original input image\n            final_detections: Final filtered detections\n            processed_proposals: Processed proposals for grid\n            scored_detections: All scored detections\n            text_query: Query text for labeling\n            save_prefix: Prefix for saved files\n        \"\"\"\n        print(\"STAGE 6: Visualization...\")\n        \n        # 1. Detection result image\n        result_image = self._draw_detections(original_image, final_detections, \n                                           f\"{self.image_count}_{save_prefix}_detection_result.jpg\")\n        \n        # 2. Enhanced grid of top proposals\n        grid_image = self._create_enhanced_grid(processed_proposals, scored_detections, \n                                              text_query, f\"{self.image_count}_{save_prefix}_enhanced_grid.jpg\")\n        \n        # 3. Analysis summary\n        self._print_analysis_summary(final_detections, scored_detections, text_query)\n        \n        return result_image, grid_image\n    \n    def _draw_detections(self, image, detections, save_path):\n        \"\"\"Draw bounding boxes on image\"\"\"\n        result = image.copy()\n        \n        for bbox, confidence in detections:\n            x_min, y_min, x_max, y_max = bbox\n            \n            # Draw red rectangle for SegGOD\n            cv2.rectangle(result, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n            \n            # Add confidence score\n            label = f\"{confidence:.2f}\"\n            cv2.putText(result, label, (x_min, y_min-10), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n        \n        # Save and display\n        result_bgr = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(save_path, result_bgr)\n        \n        plt.figure(figsize=(12, 8))\n        plt.imshow(result)\n        plt.title(f'SegGOD Detection Results - {len(detections)} detections')\n        plt.axis('off')\n        plt.show()\n        \n        print(f\"Detection result saved: {save_path}\")\n        return result\n    \n    def _create_enhanced_grid(self, processed_proposals, scored_detections, \n                            text_query, save_path, grid_size=(2, 3)):\n        \"\"\"Create enhanced visualization grid\"\"\"\n        if not processed_proposals:\n            return None\n        \n        # Sort by score (highest first)\n        combined = list(zip(processed_proposals, scored_detections))\n        sorted_combined = sorted(combined, key=lambda x: x[1][1], reverse=True)\n        \n        # Take top items for grid\n        top_items = sorted_combined[:grid_size[0] * grid_size[1]]\n        \n        # Create grid\n        segment_size = 150\n        text_height = 40\n        grid_width = grid_size[1] * segment_size\n        grid_height = grid_size[0] * (segment_size + text_height)\n        \n        canvas = np.ones((grid_height, grid_width, 3), dtype=np.uint8) * 255\n        \n        for i, ((image_pil, bbox), (_, score)) in enumerate(top_items):\n            row = i // grid_size[1]\n            col = i % grid_size[1]\n            \n            # Position in grid\n            y_start = row * (segment_size + text_height)\n            y_end = y_start + segment_size\n            x_start = col * segment_size\n            x_end = x_start + segment_size\n            \n            # Place image\n            img_array = np.array(image_pil.resize((segment_size, segment_size)))\n            canvas[y_start:y_end, x_start:x_end] = img_array\n            \n            # Add text\n            canvas_pil = Image.fromarray(canvas)\n            draw = ImageDraw.Draw(canvas_pil)\n            \n            try:\n                font = ImageFont.truetype(\"arial.ttf\", 12)\n            except:\n                font = ImageFont.load_default()\n            \n            text_y = y_end + 5\n            draw.text((x_start + 5, text_y), f\"Score: {score:.3f}\", fill=(0, 0, 0), font=font)\n            draw.text((x_start + 5, text_y + 15), f\"Query: {text_query}\", fill=(0, 0, 0), font=font)\n            \n            canvas = np.array(canvas_pil)\n        \n        # Save and display\n        canvas_bgr = cv2.cvtColor(canvas, cv2.COLOR_RGB2BGR)\n        cv2.imwrite(save_path, canvas_bgr)\n        \n        plt.figure(figsize=(12, 8))\n        plt.imshow(canvas)\n        plt.title(f'SegGOD Enhanced Grid - Top Proposals for \"{text_query}\"')\n        plt.axis('off')\n        plt.show()\n        \n        print(f\"Enhanced grid saved: {save_path}\")\n        return canvas\n    \n    def _print_analysis_summary(self, final_detections, scored_detections, text_query):\n        print(f\"\\n{'='*60}\")\n        print(\"SegGOD ANALYSIS SUMMARY\")\n        print(f\"{'='*60}\")\n        print(f\"Query: '{text_query}'\")\n        print(f\"Total proposals processed: {len(scored_detections)}\")\n        print(f\"Final detections: {len(final_detections)}\")\n        \n        if scored_detections:\n            scores = [score for _, score in scored_detections]\n            print(\"Score statistics:\")\n            print(f\" Min: {min(scores):.3f}\")\n            print(f\"Max: {max(scores):.3f}\")\n            print(f\"Mean: {np.mean(scores):.3f}\")\n            print(f\"Std: {np.std(scores):.3f}\")\n    \n    def detect_single_image(self, image, text_query, threshold=0.1):\n        \"\"\"\n        Run SegGOD detection on a single image\n        \n        Args:\n            image: Input RGB image (H×W×3)\n            text_query: Text description of target object\n            threshold: Detection confidence threshold\n            \n        Returns:\n            best_detection: Single best bounding box in format (x, y, width, height) or None\n        \"\"\"\n        # Execute all stages\n        masks = self.stage1_sam_segmentation(image)\n        proposals = self.stage2_object_proposals(image, masks)\n        processed_proposals = self.stage3_content_aware_interpolation(proposals)\n        scored_detections = self.stage4_vlm_processing(processed_proposals, text_query)\n        final_detections = self.stage5_post_processing(scored_detections, threshold)\n        self.stage6_visualization(image, final_detections, processed_proposals, scored_detections, \n            text_query)\n        \n        # Return best detection (highest confidence) in required format\n        if final_detections:\n            # Sort by confidence and take the best one\n            best_bbox, best_confidence = max(final_detections, key=lambda x: x[1])\n            \n            # Convert from (x_min, y_min, x_max, y_max) to (x, y, width, height)\n            x_min, y_min, x_max, y_max = best_bbox\n            x = x_min\n            y = y_min\n            width = x_max - x_min\n            height = y_max - y_min\n            \n            return {\"x\": x, \"y\": y, \"width\": width, \"height\": height}\n        \n        return None\n    \n    def process_image_folder(self, image_folder, output_json=\"seggod_predictions.json\",\n                             threshold=0.1, image_extensions=None):\n        \"\"\"\n        Process all images in a folder and save predictions as JSON\n        \n        Args:\n            image_folder: Path to folder containing image\n            output_json: Output JSON file path\n            threshold: Detection confidence threshold\n            image_extensions: List of valid image extensions\n            \n        Returns:\n            predictions: List of prediction dictionaries\n        \"\"\"\n        if image_extensions is None:\n            image_extensions = ['.jpg', '.jpeg', '.png']\n        \n        image_folder = Path(image_folder)\n        predictions = []\n        \n        # Get all image files\n        image_files = []\n        for ext in image_extensions:\n            image_files.extend(image_folder.glob(f\"*{ext}\"))\n            image_files.extend(image_folder.glob(f\"*{ext.upper()}\"))\n        \n        image_files = sorted(image_files)\n        \n        for i, image_path in enumerate(image_files):\n            print(f\"Processing {i+1}/{len(image_files)}: {image_path.name}\")\n            \n            try:\n                # Load image\n                image = cv2.imread(str(image_path))\n                if image is None:\n                    print(f\"Could not load image: {image_path.name}\")\n                    continue\n                \n                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n                print(\"Processing this image, FYI -->\",image_path.name)\n                text_query = input(\"Enter the text query:\")\n                bbox = self.detect_single_image(image_rgb, text_query, threshold)\n                self.image_count += 1\n                if bbox is not None:\n                    prediction = {\n                        \"image\": image_path.name,\n                        \"bbox\": bbox\n                    }\n                    predictions.append(prediction)\n                    print(f\"Detection found: {bbox}\")\n                else:\n                    print(\" No detection above threshold\")\n                \n            except Exception as e:\n                print(f\"Error processing {image_path.name}: {str(e)}\")\n                continue\n\n        print(\"Done processing all the images\")\n        print(f\"{len(predictions)}/{len(image_files)} images had detections\")\n        \n        return predictions\n    \n\n\nseggod = SegGODBatchProcessor(\n    sam_model_path=\"sam_vit_l_0b3195.pth\",\n    vlm_model=\"google/siglip-base-patch16-224\"\n)\nprint(\"Lets do this!!!!!!\")\n\n# You give the path to the folder of images\n# it iterates through each image, you enter the text query\n# it performs detection and saves the result in your directory\n# with unique names\npredictions_blah = seggod.process_image_folder(\n    image_folder=\"/kaggle/input/seggod-test\",\n    output_json=\"seggod_person_predictions.json\",\n    threshold=0.08\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}